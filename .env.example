# ============================================================================
# CHEF COMPENSWISS Scraper - Environment Configuration
# ============================================================================
# Copy this file to .env and fill in your API keys
# cp .env.example .env

# ============================================================================
# OpenRouter API Configuration
# ============================================================================
# Get your free API key at: https://openrouter.ai/keys
# Free models available: qwen/qwen-2.5-coder-32b-instruct:free,
#                        qwen/qwen-2.5-72b-instruct:free,
#                        meta-llama/llama-3.3-70b-instruct:free

OPENROUTER_API_KEY=your_openrouter_key_here

# ============================================================================
# Gemini API Configuration
# ============================================================================
# Get your free API key at: https://aistudio.google.com/app/apikey
# Free tier: 15 requests/minute, 1,500 requests/day

GEMINI_API_KEY=your_gemini_key_here

# ============================================================================
# GroqCloud API Configuration
# ============================================================================
# Get your free API key at: https://console.groq.com/keys
# Free tier: 30 requests/minute, 14,400 requests/day
# Models: llama-3.3-70b-versatile (FASTEST, most reliable)

GROQ_CLOUD_API_KEY=your_groq_key_here

# ============================================================================
# LLM Extraction Settings (Optional)
# ============================================================================
# Model to use for fallback extraction (when regex/proximity extraction fails)
# Recommended free models:
#   - qwen/qwen-2.5-coder-32b-instruct:free (BEST for structured data - 10/10 score, fastest)
#   - qwen/qwen-2.5-72b-instruct:free (Larger model, more powerful)
#   - meta-llama/llama-3.3-70b-instruct:free (Alternative, reliable)
#   - deepseek/deepseek-r1:free (Experimental, may hit rate limits)

# Primary model (best for structured data extraction)
LLM_MODEL=qwen/qwen-2.5-coder-32b-instruct:free

# Fallback models (in order of preference)
LLM_MODEL_FALLBACK_1=qwen/qwen-2.5-72b-instruct:free
LLM_MODEL_FALLBACK_2=deepseek/deepseek-r1:free
LLM_MODEL_FALLBACK_3=meta-llama/llama-3.3-70b-instruct:free
LLM_MODEL_FALLBACK_4=qwen/qwen3-coder:free

# Enable/disable LLM fallback (set to "true" or "false")
ENABLE_LLM_FALLBACK=true

# ============================================================================
# LLM Provider Priority (Fallback Order)
# ============================================================================
# Set the order in which LLM providers are tried when extraction fails
# Options: groq, openrouter, gemini
#
# Examples:
#   - "groq,openrouter,gemini" (tries GroqCloud first, then OpenRouter, then Gemini)
#   - "openrouter,gemini,groq" (tries OpenRouter first, then Gemini, then GroqCloud)
#   - "groq" (only use GroqCloud, skip others)
#   - "gemini,groq" (try Gemini first, then GroqCloud, skip OpenRouter)
#
# Recommended: "groq,openrouter,gemini" (fastest to slowest, based on test results)
#
# Performance benchmarks:
#   - GroqCloud llama-3.3-70b-versatile: 100% accuracy, 2.5s (FASTEST)
#   - OpenRouter meta-llama/llama-3.3-70b: 100% accuracy, 14s
#   - Gemini gemini-2.0-flash-exp: May hit quota limits

LLM_PROVIDER_PRIORITY=groq,openrouter,gemini

# ============================================================================
# Configuration Notes
# ============================================================================
#
# 1. You need at least ONE API key (GroqCloud recommended for best performance)
# 2. Having multiple API keys provides redundancy and fallback options
# 3. Free tier limits:
#    - GroqCloud: 30 RPM, 14.4K RPD (most generous)
#    - Gemini: 15 RPM, 1.5K RPD
#    - OpenRouter: Varies by model, may hit rate limits
#
# 4. The scraper has 3-tier extraction:
#    Tier 1: Regex patterns (fastest, works when website format is stable)
#    Tier 2: Sentence-based proximity extraction (handles minor wording changes)
#    Tier 3: LLM fallback (handles major structural changes, requires API keys)
#
# 5. Test your configuration:
#    python bin/test_llm_fallback.py
#
# 6. To disable LLM fallback completely:
#    ENABLE_LLM_FALLBACK=false
#
# ============================================================================
